{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seminar-StyleTransfer-Pytorch-Unity.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPIjOkg/yxwbJgGBv1LPlCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laure-m/IDEAS_Tech_Seminar_Winter_2022/blob/main/Seminar_StyleTransfer_Pytorch_Unity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UCLA.AUD.IDEAS TECHNOLOGY SEMINAR \n",
        "#**PYTORCH STYLETRANSFER NOTEBOOK FOR UNITY**"
      ],
      "metadata": {
        "id": "ssNQdmiExfpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT:** Read all text before running the cell as some directions can change parameters or might download to the incorrect folder \n",
        "Read the output of each cell after you run it > if you get an error or something didnt load or it cant find a folder or image then SOMETHING IS WRONG. "
      ],
      "metadata": {
        "id": "jbOSN4-N0Mtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PART 01 - NOTEBOOK ENVIRONMENT SETUP**"
      ],
      "metadata": {
        "id": "SNlMQmoD0PdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dvCPVKg6xVXW"
      },
      "outputs": [],
      "source": [
        "#@title 01:  Install fastai library (built on PyTorch)\n",
        "%%capture\n",
        "!pip install fastai==2.2.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 02:  Import Dependencies, Modules, Packages\n",
        "# Miscellaneous operating system interfaces\n",
        "# https://docs.python.org/3/library/os.html\n",
        "import os\n",
        "# Time access and conversions\n",
        "# https://docs.python.org/3/library/time.html\n",
        "import time\n",
        "# Object-oriented filesystem paths\n",
        "# https://docs.python.org/3/library/pathlib.html#pathlib.Path\n",
        "from pathlib import Path\n",
        "# Tuple-like objects that have named fields\n",
        "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
        "from collections import namedtuple\n",
        "\n",
        "# A convenience function for downloading files from a url to a destination folder\n",
        "# https://docs.fast.ai/data.external.html#untar_data\n",
        "from fastai.data.external import untar_data\n",
        "\n",
        "# Provides image processing capabilities\n",
        "# https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
        "from PIL import Image\n",
        "\n",
        "# The main PyTorch package\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "import torch\n",
        "\n",
        "# Used to iterate over the dataset during training \n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Contains definitions of models. We'll be downloading a pretrained VGG-19 model\n",
        "# to judge the performance of our style transfer model.\n",
        "# https://pytorch.org/vision/stable/models.html#torchvision.models.vgg19\n",
        "from torchvision.models import vgg19\n",
        "# Common image transforms that we'll use to process images before feeding them to the models\n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "from torchvision import transforms\n",
        "# Loads images from a directory and applies the specified transforms\n",
        "# https://pytorch.org/vision/stable/datasets.html#imagefolder\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yudDnaMv0Uri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 03:  Define some Functions\n",
        "def make_dir(dir_name: str):\n",
        "    \"\"\"Create the specified directory if it doesn't already exist\"\"\"\n",
        "    dir_path = Path(dir_name)\n",
        "    try:\n",
        "        dir_path.mkdir()\n",
        "    except:\n",
        "        print(\"Directory already exists.\")\n",
        "\n",
        "def load_image(filename: str, size: int=None, scale: float=None):\n",
        "    \"\"\"Load the specified image and return it as a PIL Image\"\"\"\n",
        "    img = Image.open(filename)\n",
        "    if size is not None:\n",
        "        img = img.resize((size, size), Image.ANTIALIAS)\n",
        "    elif scale is not None:\n",
        "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
        "    return img\n",
        "\n",
        "def save_image(filename: str, data: torch.Tensor):\n",
        "    \"\"\"Save the Tensor data to an image file\"\"\"\n",
        "    img = data.clone().clamp(0, 255).numpy()\n",
        "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(filename)\n",
        "\n",
        "def load_checkpoint(model_path):\n",
        "    state_dict = torch.load(model_path)\n",
        "    keys = [k for k in state_dict.keys()]\n",
        "    filters = set()\n",
        "    filters_list = [state_dict[k].shape[0] for k in keys if not (state_dict[k].shape[0] in filters or filters.add(state_dict[k].shape[0]))]\n",
        "    res_blocks = len(set(k.split('.')[1] for k in state_dict.keys() if 'resnets' in k))\n",
        "    model = TransformerNet(filters=filters_list[:-1], res_blocks=res_blocks) \n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "def stylize(model_path: str, input_image: str, output_image: str, content_scale: float=None, \n",
        "            device: str=\"cpu\", export_onnx: bool=None):\n",
        "    \"\"\"Load a TransformerNet checkpoint, stylize an image and save the output\"\"\"\n",
        "    device = torch.device(device)\n",
        "    content_image = load_image(input_image, scale=content_scale)\n",
        "    content_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        style_model = load_checkpoint(model_path)\n",
        "        style_model.to(device)\n",
        "         \n",
        "        if export_onnx:\n",
        "            assert export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n",
        "            output = torch.onnx._export(style_model, content_image, export_onnx, opset_version=9).cpu()\n",
        "        else:\n",
        "            output = style_model(content_image).cpu()\n",
        "    save_image(output_image, output[0])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DLZQkqKv0aOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 04:  Define the Style Transfer Model\n",
        "#@markdown Here we'll define the style transfer model itself. The model takes in an RGB image and generates a new image with the same dimensions. The features in the output image (e.g. color and texure) are then compared with the features of the style image and content image. The results of these comparisons are then used to update the parameters of the model so that it hopefully generates better images.\n",
        "\n",
        "class TransformerNet(torch.nn.Module):\n",
        "    \"\"\"TransformerNet\n",
        "    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L4\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, filters=(32, 64, 128), res_blocks=5):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.res_blocks = res_blocks if res_blocks > 0 else 1\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, filters[0], kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n",
        "        self.conv2 = ConvLayer(filters[0], filters[1], kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n",
        "        self.conv3 = ConvLayer(filters[1], filters[2], kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(filters[2], affine=True)\n",
        "        # Residual layers\n",
        "        self.resnets = torch.nn.ModuleList()\n",
        "        for i in range(self.res_blocks):\n",
        "            self.resnets.append(ResidualBlock(filters[2]))\n",
        "        \n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(filters[2], filters[1], kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(filters[1], filters[0], kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n",
        "        self.deconv3 = ConvLayer(filters[0], 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        \n",
        "    def forward(self, X):\n",
        "        conv1_y = self.relu(self.in1(self.conv1(X)))\n",
        "        conv2_y = self.relu(self.in2(self.conv2(conv1_y)))\n",
        "        conv3_y = self.relu(self.in3(self.conv3(conv2_y)))\n",
        "\n",
        "        y = self.resnets[0](conv3_y) + conv3_y\n",
        "        \n",
        "        for i in range(1, self.res_blocks):\n",
        "            y = self.resnets[i](y) + y\n",
        "\n",
        "        y = self.relu(self.in4(self.deconv1(conv3_y + y)))\n",
        "        y = self.relu(self.in5(self.deconv2(conv2_y + y)))\n",
        "        y = self.deconv3(conv1_y + y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    \"\"\"ConvLayer\n",
        "    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L44\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L57\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "      \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L79\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r-G5YkpC06UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 05:  Define the VGG-19 Model\n",
        "#@markdown Next, we'll define the model that will be used to judge the quality of the output images from the style transfer model. This model has been pretrained a large image dataset. We'll use this model to extract the features of the content image, style image, and stylized images.\n",
        "class Vgg19(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/vgg.py#L7\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        self.feature_layers = [0, 3, 5]\n",
        "        self.vgg_pretrained_features = vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), self.vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), self.vgg_pretrained_features[x])\n",
        "        for x in range(9, 18):\n",
        "            self.slice3.add_module(str(x), self.vgg_pretrained_features[x])\n",
        "        for x in range(18, 27):\n",
        "            self.slice4.add_module(str(x), self.vgg_pretrained_features[x])\n",
        "        for x in range(27, 36):\n",
        "            self.slice5.add_module(str(x), self.vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "            \n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "        h = self.slice5(h)\n",
        "        h_relu5_3 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
        "        return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3Y_SXuu60-03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 06:  Define the Model Trainer\n",
        "#@markdown This is still setting everything up\n",
        "class Trainer(object):\n",
        "    def __init__(self, train_loader, style_transform, generator, opt_generator, style_criterion, perception_model, device):\n",
        "        self.train_loader = train_loader\n",
        "        self.style_transform = style_transform\n",
        "        self.generator = generator\n",
        "        self.opt_generator = opt_generator\n",
        "        self.style_criterion = style_criterion\n",
        "        self.perception_model = perception_model\n",
        "        self.device = device\n",
        "        self.generator.to(self.device)\n",
        "        \n",
        "    def gram_matrix(self, y: torch.Tensor):\n",
        "        \"\"\"Compute the gram matrix a PyTorch Tensor\"\"\"\n",
        "        (b, ch, h, w) = y.size()\n",
        "        features = y.view(b, ch, w * h)\n",
        "        features_t = features.transpose(1, 2)\n",
        "        gram = features.bmm(features_t) / (ch * h * w)\n",
        "        return gram\n",
        "\n",
        "    def normalize_batch(self, batch: torch.Tensor):\n",
        "        \"\"\"Normalize a batch of Tensors using the imagenet mean and std \"\"\"\n",
        "        mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "        std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "        batch = batch.div_(255.0)\n",
        "        return (batch - mean) / std\n",
        "\n",
        "    def get_gram_style(self, style_image: str, style_size: int):\n",
        "        \"\"\"Get the Gram Matrices for the style image\"\"\"\n",
        "        style = load_image(style_image, size=style_size)\n",
        "        style = self.style_transform(style)\n",
        "        style = style.repeat(self.train_loader.batch_size, 1, 1, 1).to(self.device)\n",
        "        features_style = self.perception_model(self.normalize_batch(style))\n",
        "        gram_style = [self.gram_matrix(y) for y in features_style]\n",
        "        return gram_style\n",
        "            \n",
        "    def save_checkpoint(self, path: str):\n",
        "        \"\"\"Save the current model weights at the specified path\"\"\"\n",
        "        self.generator.eval().cpu()\n",
        "        torch.save(self.generator.state_dict(), path)\n",
        "        print(f\"Checkpoint saved at {path}\")\n",
        "\n",
        "    def train(self, style_image, test_image, checkpoint_model_dir, epochs=5, content_weight=1e5, style_weight=1e10, \n",
        "                content_scale=None, style_size=None, log_interval=500, checkpoint_interval=500):\n",
        "        \"\"\"Train the style transfer model on the provided style image.\"\"\"\n",
        "        \n",
        "        gram_style = self.get_gram_style(style_image, style_size)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            self.generator.train()\n",
        "            agg_content_loss = 0.\n",
        "            agg_style_loss = 0.\n",
        "            count = 0\n",
        "            for batch_id, (x, _) in enumerate(self.train_loader):\n",
        "                n_batch = len(x)\n",
        "                count += n_batch\n",
        "                self.opt_generator.zero_grad()\n",
        "                \n",
        "                x = x.to(self.device)\n",
        "                y = self.generator(x)\n",
        "\n",
        "                y = self.normalize_batch(y.clone())\n",
        "                x = self.normalize_batch(x.clone())\n",
        "                features_y = self.perception_model(y)\n",
        "                features_x = self.perception_model(x)\n",
        "\n",
        "                content_loss = content_weight * self.style_criterion(features_y.relu2_2, features_x.relu2_2)\n",
        "\n",
        "                style_loss = 0.\n",
        "                for ft_y, gm_s in zip(features_y, gram_style):\n",
        "                    gm_y = self.gram_matrix(ft_y)\n",
        "                    style_loss += self.style_criterion(gm_y, gm_s[:n_batch, :, :])\n",
        "                style_loss = style_loss * style_weight\n",
        "\n",
        "                total_loss = content_loss + style_loss\n",
        "                total_loss.backward()\n",
        "                self.opt_generator.step()\n",
        "\n",
        "                agg_content_loss += content_loss.item()\n",
        "                agg_style_loss += style_loss.item()\n",
        "\n",
        "                if (batch_id + 1) % log_interval == 0:\n",
        "                    mesg = f\"{' '.join(time.ctime().replace('  ', ' ').split(' ')[1:-1])}  \"\n",
        "                    mesg += f\"Epoch {e + 1}: [{count}/{len(self.train_loader.dataset)}]  \"\n",
        "                    mesg += f\"content: {(agg_content_loss / (batch_id + 1)):.4f}  \"\n",
        "                    mesg += f\"style: {(agg_style_loss / (batch_id + 1)):.4f}  \"\n",
        "                    mesg += f\"total: {((agg_content_loss + agg_style_loss) / (batch_id + 1)):.4f}\"\n",
        "                    print(mesg)\n",
        "\n",
        "                if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
        "                    ckpt_base = f\"ckpt_epoch_{e}_batch_id_{batch_id + 1}\"\n",
        "                    ckpt_model_filename = ckpt_base + \".pth\"\n",
        "                    ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n",
        "                    self.save_checkpoint(ckpt_model_path)\n",
        "                    output_image = ckpt_base + \".png\"\n",
        "                    output_image_path = os.path.join(checkpoint_model_dir, output_image)\n",
        "                    stylize(ckpt_model_path, test_image, output_image_path)\n",
        "                    self.generator.to(self.device).train()\n",
        "                \n",
        "        print(\"Finished Training\")\n",
        "        ckpt_model_path = os.path.join(checkpoint_model_dir, 'final.pth')\n",
        "        self.save_checkpoint(ckpt_model_path)\n",
        "        output_image_path = os.path.join(checkpoint_model_dir, 'final.png')\n",
        "        stylize(ckpt_model_path, test_image, output_image_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nsdWqHjs1NQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PART 02 - FOLDER SETUP**"
      ],
      "metadata": {
        "id": "bg2hEK771QwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 01:  Mount to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DykHQOHV1tyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 02: Make a Folder in your drive named `styleTransfer-Pytorch`\n",
        "#@markdown And sets some folder directories\n",
        "%cd /content/gdrive/MyDrive\n",
        "!mkdir styleTransfer-Pytorch\n",
        "%cd /content/gdrive/MyDrive/styleTransfer-Pytorch\n",
        "!mkdir style_images\n",
        "!mkdir test_images\n",
        "!mkdir dataset\n",
        "!mkdir checkpoints\n",
        "!mkdir ./dataset/video_frames/\n",
        "\n",
        "dataset_dir = \"/content/dataset\"\n",
        "\n",
        "project_dir = '/content/gdrive/MyDrive/styleTransfer-Pytorch'\n",
        "style_images_dir = f\"{project_dir}/style_images\"\n",
        "test_images_dir = f\"{project_dir}/test_images\"\n",
        "checkpoints_dir = f\"{project_dir}/checkpoints\"\n",
        "make_dir(checkpoints_dir)\n",
        "\n",
        "!ls $project_dir"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CsiT9ap01xUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now save test images and style images in their respective folders. Style images and test images need to be JPG and the dataset images can be either png or jpg"
      ],
      "metadata": {
        "id": "9r0zYC7P4jFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 03:  Download the COCO Training Dataset\n",
        "#@markdown We'll be using the COCO train 2014 image dataset to train. It contains `82,783` images.     \n",
        "#@markdown Note: It's about 13.5 GB unzipped. You might get a disk space warning while the dataset is being unzipped. You can click ignore in the popup window.  \n",
        "coco_url = 'http://images.cocodataset.org/zips/train2014.zip'\n",
        "untar_data(coco_url, 'coco.zip', dataset_dir)\n",
        "if os.path.exists('coco.zip'): os.remove('coco.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mr_-fjw91_Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 04:  Set some parameters for the model\n",
        "batch_size = 4\n",
        "image_size = 256\n",
        "transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Lambda(lambda x: x.mul(255))\n",
        "                                ])\n",
        "\n",
        "train_dataset = ImageFolder(dataset_dir, transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "use_cuda = True\n",
        "device = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\n",
        "print(f\"Using: {device}\")\n",
        "\n",
        "style_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Lambda(lambda x: x.mul(255))\n",
        "                                      ])\n",
        "\n",
        "filters = (8, 16, 32)\n",
        "res_blocks = 5\n",
        "generator = TransformerNet(filters=filters, res_blocks=res_blocks).to(device)\n",
        "\n",
        "lr = 1e-3\n",
        "opt_generator = torch.optim.Adam(generator.parameters(), lr)\n",
        "\n",
        "style_criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P_-F2Ao92KYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 05:  Create a New VGG-19 Perception Model\n",
        "perception_model = Vgg19(requires_grad=False).to(device)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "npwxzlen2e56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PART 03: TRAINING SECTION**"
      ],
      "metadata": {
        "id": "DSQaKqgW2hE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 01: Create a New Trainer \n",
        "trainer = Trainer(train_loader=train_loader, \n",
        "                  style_transform=style_transform, \n",
        "                  generator=generator, \n",
        "                  opt_generator=opt_generator, \n",
        "                  style_criterion=style_criterion, \n",
        "                  perception_model=perception_model, \n",
        "                  device=device)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N4jbvQMG3SIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 02: Set Training Parameters\n",
        "#@markdown Make sure the style_image and test_image are both .jpg and 256x256       \n",
        "\n",
        "#@markdown Change the style_image, epochs, and style_weight\n",
        "\n",
        "# The file path for the target style image\n",
        "style_image = f\"{style_images_dir}/Lebb1.jpg\"\n",
        "# The file path for a sample input image for demonstrating the model's progress during training\n",
        "test_image = f\"{test_images_dir}/mesh.jpg\" \n",
        "\n",
        "# The number of times to iterate through the entire training dataset\n",
        "epochs = 6\n",
        "\n",
        "# The influence from the input image on the stylized image - Default: 1e5\n",
        "content_weight = 1e5 \n",
        "# The influence from the style image on the stylized image - Default: 1e10\n",
        "style_weight = 2e10\n",
        "\n",
        "# (test_image resolution) / content_scale\n",
        "content_scale = 2.0 #1\n",
        "# Target size for style_image = (style_size, styl_size)\n",
        "style_size = 256\n",
        "\n",
        "# The number of training batches to wait before printing the progress of the model \n",
        "log_interval = 500\n",
        "# The number of training to wait before saving the current model weights\n",
        "checkpoint_interval = 500"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RtSjYCIP3Tw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 03: TRAIN\n",
        "trainer.train(style_image=style_image, \n",
        "              test_image=test_image, \n",
        "              checkpoint_model_dir=checkpoints_dir, \n",
        "              epochs=epochs, \n",
        "              content_weight=content_weight, \n",
        "              style_weight=style_weight,\n",
        "              content_scale=content_scale,\n",
        "              style_size=style_size,\n",
        "              log_interval=log_interval, \n",
        "              checkpoint_interval=checkpoint_interval)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SbtB8J_C3V_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PART 04: EXPORT FOR UNITY**"
      ],
      "metadata": {
        "id": "OxGIokjw3YJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export ONNX File >> Unity\n",
        "#@markdown Make sure to change the checkpoint .pth name and the exported onnx file name to something that relates to the style so you remember.       \n",
        "#@markdown The ONNX file will be saved to the project folder in Google Drive.\n",
        "checkpoint_path = f\"{checkpoints_dir}/ckpt_epoch_1_batch_id_19000.pth\"\n",
        "style_model = load_checkpoint(checkpoint_path)\n",
        "x = torch.randn(1, 3, 960, 540).cpu()\n",
        "\n",
        "torch.onnx.export(style_model.cpu(),     #  Model being run\n",
        "                  x,                           # Sample input\n",
        "                  f\"{project_dir}/bugView.onnx\", # Path to save ONNX file\n",
        "                  export_params=True,          # Store trained weights\n",
        "                  opset_version=9,             # Which ONNX version to use\n",
        "                  do_constant_folding=True     # Replace operations that have all constant inputs with pre-computed nodes\n",
        "                 )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dbLRUVdQ3ZjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now drag this ONNX file into your StyleTransfer > models folder and apply it to the camera's style transfer script!"
      ],
      "metadata": {
        "id": "rNiuvU2-3d9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "///\n",
        "\n",
        "This notebook was put together by Laure Michelon for the WInter Quarter Tech Seminar @UCLA IDEAS\n",
        "\n",
        "github - https://github.com/laure-m"
      ],
      "metadata": {
        "id": "zcSQuwG55sOF"
      }
    }
  ]
}